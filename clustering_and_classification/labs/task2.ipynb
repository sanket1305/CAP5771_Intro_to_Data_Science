{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e8e846-0dc5-489b-8a1d-b73f1e58d1a4",
   "metadata": {},
   "source": [
    "# Task 2: TF-IDF Representation\n",
    "\n",
    "## Overview\n",
    "In this task, you will enhance the **Bag of Words (BoW) representation** by applying **Term Frequency-Inverse Document Frequency (TF-IDF) weighting.** This transformation helps improve text classification by reducing the influence of common words while highlighting more important terms.\n",
    "\n",
    "## Why TF-IDF?\n",
    "\n",
    "The **TF-IDF model** is an improvement over BoW. Instead of just counting word occurrences, it:\n",
    "\n",
    "1. Assigns **higher weights** to words that appear frequently in a document but **rarely in other documents** (important keywords).\n",
    "2. Assigns **lower weights** to words that appear in **many documents** (e.g., \"the\", \"is\", “and”), as they contribute less to distinguishing meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295f062b-10eb-4129-8386-c8470051ec67",
   "metadata": {},
   "source": [
    "## How TF-IDF Works\n",
    "\n",
    "### **1. Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures how often a term (word) appears in a document. Words that appear more frequently in a document are considered more important.\n",
    "\n",
    "The formula for calculating TF is:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t) = \\frac{\\text{Number of times term } t \\text{ appears in a document}}{\\text{Total number of terms in the document}}\n",
    "$$\n",
    "\n",
    "### **2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is across an entire corpus (collection of documents). Terms that appear in many documents have lower IDF scores because they are less specific to any single document.\n",
    "\n",
    "The formula for IDF is:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t}\\right)\n",
    "$$\n",
    "\n",
    "To avoid division by zero when a term appears in all documents, some implementations use a modified formula:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing term } t} + 1\\right)\n",
    "$$\n",
    "\n",
    "### **3. TF-IDF Score**\n",
    "\n",
    "The TF-IDF score combines Term Frequency and Inverse Document Frequency to assign importance to terms. It gives higher importance to terms that are frequent in a specific document but rare across the corpus.\n",
    "\n",
    "The formula for calculating TF-IDF is:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t) = \\text{TF}(t) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "### **Why Use TF-IDF?**\n",
    "\n",
    "TF-IDF is widely used to represent textual data as numerical vectors where each dimension corresponds to a word in the corpus. It helps identify significant words within a document while reducing the influence of commonly occurring words across all documents.\n",
    "\n",
    "### **Applications of TF-IDF**\n",
    "\n",
    "- Text classification\n",
    "- Clustering\n",
    "- Search engine indexing\n",
    "- Information retrieval systems\n",
    "- Document similarity analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58bdc6b5-0322-4217-9688-48d71a7e41ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dacf221-0348-4bf6-9139-53c2f760a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# tfidf for train dataset\n",
    "X_train = np.load('data/bow_train.npy')\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(X_train).toarray()\n",
    "np.save('data/tfidf_train.npy', x_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b841036c-39f7-498b-b6ff-5b87d8a29789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf for test dataset \n",
    "X_test = np.load('data/bow_test.npy')\n",
    "x_test_tfidf = tfidf_transformer.transform(X_test).toarray()\n",
    "np.save('data/tfidf_test.npy', x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d6d534-d161-462b-bc8a-be1779daabbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF train shape: 11314 x 1000\n",
      "TF-IDF test shape: 7532 x 1000\n"
     ]
    }
   ],
   "source": [
    "# display dimension for each of them\n",
    "print(\"TF-IDF train shape: \" + str(x_train_tfidf.shape[0]) + \" x \" + str(x_train_tfidf.shape[1]))\n",
    "print(\"TF-IDF test shape: \" + str(x_test_tfidf.shape[0]) + \" x \" + str(x_test_tfidf.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
